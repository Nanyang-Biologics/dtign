{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf171c2",
   "metadata": {},
   "source": [
    "# Search suitable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a7740f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home1/yueming/Drug_Discovery/Datasets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m least_sample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1600\u001b[39m\n\u001b[1;32m      6\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home1/yueming/Drug_Discovery/Datasets/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_dir):\n\u001b[1;32m      8\u001b[0m     target_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, target)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m assay \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(target_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home1/yueming/Drug_Discovery/Datasets/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "interested_assay = 'IC50_in_nM'\n",
    "least_sample_num = 1600\n",
    "dataset_dir = '/home1/yueming/Drug_Discovery/Datasets/'\n",
    "for target in os.listdir(dataset_dir):\n",
    "    target_path = os.path.join(dataset_dir, target)\n",
    "    for assay in os.listdir(target_path):\n",
    "        if assay == interested_assay:\n",
    "            assay_path = os.path.join(target_path, assay)\n",
    "            for data_file in os.listdir(assay_path):\n",
    "                if data_file.split('.')[0][-3:] == 'all':\n",
    "                    df_path = os.path.join(assay_path, data_file)\n",
    "                    df = pd.read_csv(df_path)\n",
    "                    df_len = len(df)\n",
    "                    if df_len >= least_sample_num:\n",
    "                        print(target, assay, df_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca6523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "interested_assay = 'IC50_in_nM'\n",
    "least_sample_num, largest_sample_num = 1600, 2000\n",
    "dataset_dir = '/home1/yueming/Drug_Discovery/Datasets/'\n",
    "for target in os.listdir(dataset_dir):\n",
    "    target_path = os.path.join(dataset_dir, target)\n",
    "    for assay in os.listdir(target_path):\n",
    "        if assay == interested_assay:\n",
    "            assay_path = os.path.join(target_path, assay)\n",
    "            for data_file in os.listdir(assay_path):\n",
    "                if data_file.split('.')[0][-3:] == 'all':\n",
    "                    df_path = os.path.join(assay_path, data_file)\n",
    "                    df = pd.read_csv(df_path)\n",
    "                    df_len = len(df)\n",
    "                    if df_len >= least_sample_num and df_len <= largest_sample_num:\n",
    "                        print(target, assay, df_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c86ff",
   "metadata": {},
   "source": [
    "# Standardize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d22b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# target id: [assay type, #compounds, #binding sites, #pockets, UniPort id, potein name (all human)]\n",
    "target_assay_dict = {'CHEMBL3820': ['EC50_in_nM', 997, 11, None, 'P35557', 'Hexokinase-4'], \n",
    "                     'CHEMBL4422': ['EC50_in_nM', 1693, 2, None, 'O14842', 'Free fatty acid receptor 1'], \n",
    "                     'CHEMBL235': ['EC50_in_nM', 3611, 4, None, 'P37231', 'Peroxisome proliferator-activated receptor gamma'],\n",
    "                     'CHEMBL202': ['IC50_in_nM', 957, 8, 7, 'P00374','Dihydrofolate reductase'], \n",
    "                     'CHEMBL3976': ['IC50_in_nM', 1642, 3, None, 'Q9UHL4','Dipeptidyl peptidase 2'], \n",
    "                     'CHEMBL333': ['IC50_in_nM', 3686, 24, None, 'P08253','72 kDa type IV collagenase'], \n",
    "                     'CHEMBL2971': ['IC50_in_nM', 6207, 8, None, 'O60674','JAK2_HUMAN'], \n",
    "                     'CHEMBL279': ['IC50_in_nM', 9573, 4, None, 'P35968','Vascular endothelial growth factor receptor 2']}\n",
    "task_name_convert_dict = {'EC50_in_nM': 'pEC50', 'IC50_in_nM': 'pIC50'}\n",
    "row_activity_root = '/home1/yueming/Drug_Discovery/OneDrive_1_2022-12-13/Table 2 - Done/'\n",
    "query_root = '/home1/yueming/Drug_Discovery/Datasets/'\n",
    "output_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/dataset/'\n",
    "set_list = ['all', 'test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "\n",
    "def re_normalize(source_df, query_path, set_name, output_path, output_assay_name):\n",
    "    new_column_names = {'value': output_assay_name, 'smiles': 'SMILES', 'ChEMBL_Compound_ID': 'ChEMBL_Compound_ID'}\n",
    "    # 读取CSV文件\n",
    "    query_df = pd.read_csv(query_path)\n",
    "    file_path = query_path.replace('all', set_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 获取分子SMILES列的数据\n",
    "    smi_column_name = 'SMILES' if set_name == 'all' else 'smiles'\n",
    "    value_column_name = 'Standard_Value' if set_name == 'all' else 'value'\n",
    "    smiles_list = df[smi_column_name].tolist()\n",
    "    \n",
    "    for smiles in tqdm(smiles_list):\n",
    "        ChEMBL_Compound_ID = query_df[query_df['SMILES'].values == smiles].ChEMBL_Compound_ID.values\n",
    "        single_df = source_df[source_df['ChEMBL Compound ID'].values == ChEMBL_Compound_ID]\n",
    "        row_assay_values = single_df['Standard Value'].values\n",
    "        assay_values = [x for x in row_assay_values if not math.isnan(x)]\n",
    "        assay_value = np.mean(assay_values)\n",
    "        standard_assay_value = - np.log10(assay_value) + 9  # 1 M = 10^9 nM\n",
    "        row_indices = df[df[smi_column_name].values == smiles].index\n",
    "        df.loc[row_indices, value_column_name] = standard_assay_value\n",
    "        df.loc[row_indices, 'ChEMBL_Compound_ID'] = ChEMBL_Compound_ID\n",
    "    if set_name == 'all':\n",
    "        df['Standard_Type'] = output_assay_name\n",
    "        del df['Standard_Units']\n",
    "        for s in set_list[1:]:\n",
    "            subset_df = pd.read_csv(query_path.replace('all', s))\n",
    "            for smi in subset_df['smiles']:\n",
    "                row_indice = df[df['SMILES'].values == smi].index\n",
    "                df.loc[row_indice, 'Subset'] = s\n",
    "    else:\n",
    "        df = df.rename(columns=new_column_names)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "for key, value in tqdm(target_assay_dict.items()):\n",
    "    task_name = \"_\".join([key, value[0]])\n",
    "    output_assay_name = task_name_convert_dict[value[0]]\n",
    "    source_df = pd.read_csv(row_activity_root + f'{key} - table 2.csv')\n",
    "    query_path = query_root + key + f'/{value[0]}/{task_name}' + '_all.csv'\n",
    "    output_dir = output_root + f'{key}/{output_assay_name}/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for set_name in set_list:\n",
    "        output_path = output_dir + f'{key}_{output_assay_name}_{set_name}.csv'\n",
    "        if key != 'CHEMBL202':\n",
    "            print(f'Processing {key} {output_assay_name} {set_name}')\n",
    "            re_normalize(source_df, query_path, set_name, output_path, output_assay_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc86dcc",
   "metadata": {},
   "source": [
    "# Read and save docking results for QVina-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "\n",
    "# List of input SDF files\n",
    "result_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/CHEMBL202/1boz/qvina_pocket/'\n",
    "output_csv_dir = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/CHEMBL202/1boz/qvina_pocket/'\n",
    "input_sets = os.listdir(result_root)\n",
    "\n",
    "# 定义正则表达式模式，只取top-1结果\n",
    "pattern = r'^\\s+1\\s+(-?\\d+\\.\\d+)'\n",
    "pattern_ = r'^\\s+1\\s+(-?\\d+)'\n",
    "\n",
    "def extract_smiles_from_pdbqt(file_path):\n",
    "    smiles = None\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('REMARK SMILES'):\n",
    "                smiles = line.split('REMARK SMILES')[1].strip()\n",
    "                break\n",
    "    return smiles\n",
    "\n",
    "for set_name in tqdm(input_sets):\n",
    "    result_dir = result_root + set_name + '/'\n",
    "    # 创建一个空的 DataFrame\n",
    "    df = pd.DataFrame(columns=['ChEMBL_Compound_ID', 'SMILES', 'Vina_Score_1', 'Vina_Score_2', 'Vina_Score_3', 'Vina_Score_4', \n",
    "                               'Vina_Score_5', 'Vina_Score_6', 'Vina_Score_7'])\n",
    "    for input_file in os.listdir(result_dir):\n",
    "        if input_file[-5:] == 'pdbqt':\n",
    "            file_name = input_file[:-6]\n",
    "            cpd = file_name.split('_')[0]\n",
    "            pocket = file_name.split('_')[-1]\n",
    "            # 读取文本文件\n",
    "            txt_path = result_dir + file_name + '.txt' # need to be coverted to \"input_file.replace('pdbqt', 'txt')\" after being corrected\n",
    "            with open(txt_path, 'r') as txt:\n",
    "                lines = txt.readlines()\n",
    "            smiles = extract_smiles_from_pdbqt(result_dir + input_file)\n",
    "            # 提取符合模式的行，并保存到列表中\n",
    "            for line in lines:\n",
    "                match = re.findall(pattern, line, flags=re.MULTILINE)\n",
    "                if match:\n",
    "                    affinity = float(match[0])\n",
    "                    break\n",
    "            if cpd in df['ChEMBL_Compound_ID'].values:\n",
    "                condition = (df['ChEMBL_Compound_ID']==cpd)\n",
    "                df.loc[condition, f'Vina_Score_{pocket}'] = affinity\n",
    "            else:\n",
    "                init_content_list = [cpd, smiles] + [None] * 7\n",
    "                df.loc[len(df)] = init_content_list\n",
    "                df.loc[len(df)-1, f'Vina_Score_{pocket}'] = affinity\n",
    "\n",
    "    # 保存更新后的CSV表格\n",
    "    df.to_csv(output_csv_dir + f'{set_name}/vina_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f1fc1",
   "metadata": {},
   "source": [
    "# Save Vina scores into the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "\n",
    "# List of input SDF files\n",
    "result_dir = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/CHEMBL202/1boz/qvina_pocket/'\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "csv_dir = f'{result_dir}/CHEMBL202_pIC50_'\n",
    "output_file = csv_dir\n",
    "    \n",
    "# 定义正则表达式模式\n",
    "pattern = r'^\\s+(1)\\s+(-?\\d+\\.\\d+)'\n",
    "pattern_ = r'^\\s+(1)\\s+(-?\\d+)'\n",
    "# new_column_names = {'value': 'pIC50', 'smiles': 'SMILES', 'ChEMBL_Compound_ID': 'ChEMBL_Compound_ID'}\n",
    "\n",
    "for dataset in tqdm(set_list):\n",
    "    dataset_path = result_dir + dataset + '/'\n",
    "    dataset_df = pd.read_csv(csv_dir + dataset + '.csv')\n",
    "#     dataset_df = dataset_df.rename(columns=new_column_names)\n",
    "    # Loop through input files\n",
    "    for input_file in os.listdir(dataset_path):\n",
    "        if input_file[-5:] == 'pdbqt':\n",
    "            # 读取文本文件\n",
    "            file_name = input_file[:-6]\n",
    "            cpd = file_name.split('_')[0]\n",
    "            pocket = file_name.split('_')[-1]\n",
    "            txt_path = dataset_path + file_name + '.txt'\n",
    "            with open(txt_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # 提取符合模式的行，并保存到列表中\n",
    "            for line in lines:\n",
    "                match_ = re.match(pattern, line)\n",
    "                match__ = re.match(pattern_, line)\n",
    "                match = match_ if match_ else match__\n",
    "                if match:\n",
    "                    number = int(match.group(1))\n",
    "                    affinity = float(match.group().rsplit(maxsplit=1)[-1])\n",
    "                    # 设置条件\n",
    "                    condition = (dataset_df['ChEMBL_Compound_ID'] == cpd)  # 示例条件，可根据实际情况修改\n",
    "                    # 根据条件筛选满足条件的行索引\n",
    "                    row_indices = dataset_df[condition].index\n",
    "                    column_name = f'Pocket_{pocket}_Vina_Score'  # 列名，根据文件名索引生成\n",
    "                    dataset_df.loc[row_indices, column_name] = affinity\n",
    "\n",
    "    # 保存更新后的CSV表格\n",
    "    dataset_df.to_csv(output_file + dataset + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d44edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "\n",
    "data_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/'\n",
    "task_dict = {0: ('CHEMBL202', 'pIC50'), 1: ('CHEMBL235', 'pEC50'), 2: ('CHEMBL279', 'pIC50'), 3: ('CHEMBL2971', 'pIC50'), \n",
    "             4: ('CHEMBL333', 'pIC50'), 5: ('CHEMBL3820', 'pEC50'), 6: ('CHEMBL3976', 'pIC50'), 7: ('CHEMBL4422', 'pEC50')}\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "\n",
    "for key, value in task_dict.items():\n",
    "    target, assay = value\n",
    "    # List of input SDF files\n",
    "    input_dir = f'{data_root}/{target}/ligand/sdf/'\n",
    "    output_dir_addH = input_dir\n",
    "    output_dir_pdbqt = f'{data_root}/{target}/ligand/pdbqt/'\n",
    "\n",
    "    # Loop through input files\n",
    "    for set_name in tqdm(set_list):\n",
    "        input_files = os.listdir(input_dir + set_name)\n",
    "        for input_file in tqdm(input_files):\n",
    "            input_file_path = input_dir + set_name + '/' + input_file\n",
    "            # Load the molecule from the SDF file\n",
    "    #         mol = Chem.SDMolSupplier(input_file_path)[0]\n",
    "\n",
    "    #         # Add explicit hydrogens\n",
    "    #         mol = Chem.AddHs(mol)\n",
    "\n",
    "    #         # Save the modified molecule with explicit hydrogens\n",
    "    #         sdf_addH_save_path = output_dir_addH + set_name + '/'\n",
    "    #         if not os.path.exists(sdf_addH_save_path):\n",
    "    #                 os.makedirs(sdf_addH_save_path)\n",
    "    #         Chem.SDWriter(sdf_addH_save_path + input_file).write(mol)\n",
    "\n",
    "            # Construct the command\n",
    "            pdbqt_save_path = output_dir_pdbqt + set_name + '/'\n",
    "            if not os.path.exists(pdbqt_save_path):\n",
    "                    os.makedirs(pdbqt_save_path)\n",
    "            command = f\"mk_prepare_ligand.py -i {input_file_path} -o {pdbqt_save_path + input_file[:-4]}.pdbqt\"\n",
    "\n",
    "            # Execute the command\n",
    "            !{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c8bbf",
   "metadata": {},
   "source": [
    "# Preprocessing PARP1 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052eca6",
   "metadata": {},
   "source": [
    "## Generate table from the web scrapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41593c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "input_file = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data.txt'\n",
    "output_file = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data.csv'\n",
    "output_txt = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data_Rowwise.txt'\n",
    "\n",
    "# 打开文件并读取文本\n",
    "with open(input_file, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 在每个\"COMPOUND\"之前添加换行符\n",
    "text = text.replace(\"COMPOUND:\", \"\\nCOMPOUND:\")\n",
    "\n",
    "# 打开目标文件并写入修改后的文本内容\n",
    "with open(output_txt, 'w') as file:\n",
    "    file.write(text)\n",
    "\n",
    "print(\"修改后的文本已保存到新文件。\")\n",
    "\n",
    "# 用于存储数据的列表\n",
    "data = {'COMPOUND': [], 'ASSAY TYPE': [], 'IC50': [], 'PUBMED': [], 'Source': []}\n",
    "\n",
    "# 正则表达式用于匹配包含PUBMED的行\n",
    "pattern_with_pubmed = r'COMPOUND: (\\d+) ASSAY TYPE (\\w+) IC50 (.*?)nM PUBMED (\\d+) Source: ([\\w]+)'\n",
    "\n",
    "# 正则表达式用于匹配不包含PUBMED的行\n",
    "pattern_without_pubmed = r'COMPOUND: (\\d+) ASSAY TYPE (\\w+) IC50 (.*?)nM Source: ([\\w]+)'\n",
    "\n",
    "matches_with_pubmed = re.finditer(pattern_with_pubmed, text, flags=re.IGNORECASE)\n",
    "matches_without_pubmed = re.finditer(pattern_without_pubmed, text, flags=re.IGNORECASE)\n",
    "\n",
    "# 处理包含PUBMED的行\n",
    "for match in matches_with_pubmed:\n",
    "    print(\"Matched line:\", match.group(0))  # 打印整行的匹配内容\n",
    "    data['COMPOUND'].append(match.group(1))\n",
    "    data['ASSAY TYPE'].append(match.group(2))\n",
    "    data['IC50'].append(match.group(3)+'nM')\n",
    "    data['PUBMED'].append(match.group(4))\n",
    "    data['Source'].append(match.group(5))\n",
    "\n",
    "# 处理不包含PUBMED的行\n",
    "for match in matches_without_pubmed:\n",
    "    print(\"Matched line:\", match.group(0))  # 打印整行的匹配内容\n",
    "    data['COMPOUND'].append(match.group(1))\n",
    "    data['ASSAY TYPE'].append(match.group(2))\n",
    "    data['IC50'].append(match.group(3)+'nM')\n",
    "    data['PUBMED'].append('')  # 如果没有PUBMED，将PUBMED列设置为空字符串\n",
    "    data['Source'].append(match.group(4))\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "lines = text.split('\\n')\n",
    "print(\"文本行数:\", len(lines)-1)\n",
    "print(\"表格行数:\", len(df))\n",
    "\n",
    "# 保存为CSV文件\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"数据已保存为CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9b730",
   "metadata": {},
   "source": [
    "## Search and save SMILES according to PUBMED ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b11f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义输入CSV文件路径和输出CSV文件路径\n",
    "input_csv = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data.csv'\n",
    "output_csv = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data_With_SMILES.csv'\n",
    "smiles_csv = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/parp1_ecfp_integer.csv'\n",
    "\n",
    "# 读取输入CSV文件\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 读取包含SMILES的CSV文件\n",
    "smiles_df = pd.read_csv(smiles_csv)\n",
    "\n",
    "# 创建一个字典，用于将COMPOUND_ID映射到SMILES\n",
    "compound_id_to_smiles = dict(zip(smiles_df['COMPOUND_ID'], smiles_df['SMILES']))\n",
    "\n",
    "# 根据COMPOUND列匹配SMILES并添加到新的SMILES列\n",
    "df['SMILES'] = df['COMPOUND'].map(compound_id_to_smiles)\n",
    "\n",
    "# 统计匹配到了多少行和没有匹配到多少行\n",
    "matched_rows = df['SMILES'].count()\n",
    "unmatched_rows = len(df) - matched_rows\n",
    "\n",
    "# 保存包含新SMILES列的CSV文件\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# 打印统计信息\n",
    "print(f\"总共行数: {len(df)}\")\n",
    "print(f\"匹配到的行数: {matched_rows}\")\n",
    "print(f\"未匹配到的行数: {unmatched_rows}\")\n",
    "print(\"已添加SMILES列并保存为CSV文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18089fc",
   "metadata": {},
   "source": [
    "## Split to data subsets according to their Bemis-Murcko scaffolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d5972",
   "metadata": {},
   "source": [
    "## Distributions of Bemis-Murcko scaffolds in data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c142d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置Matplotlib字体大小\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# 读取CSV文件\n",
    "input_csv = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/Web_Scrapper_Data_With_ALL_SMILES.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 定义一个函数来提取IC50值并将其转换为浮点数\n",
    "def extract_ic50(ic50_str):\n",
    "    ic50_match = re.search(r'[<>]?=?\\s*(\\d+(\\.\\d+)?)\\s*[nN][mM]', ic50_str)\n",
    "    if ic50_match:\n",
    "        return float(ic50_match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 添加新的pIC50列\n",
    "df['pIC50'] = df['IC50'].apply(lambda x: 9 - np.log10(extract_ic50(x)) if extract_ic50(x) else None)\n",
    "\n",
    "# 计算每个COMPOUND的pIC50中位数\n",
    "df['Median_pIC50'] = df.groupby('COMPOUND')['pIC50'].transform('median')\n",
    "\n",
    "# 选择中位数最高的行\n",
    "df = df[df['pIC50'] == df['Median_pIC50'].fillna(-np.inf)]\n",
    "\n",
    "# 标记重复行（保留每个COMPOUND组的第一行）\n",
    "df['IsDuplicate'] = df.duplicated(subset=['COMPOUND'], keep='first')\n",
    "\n",
    "# 选择IsDuplicate为False的行\n",
    "df = df[~df['IsDuplicate']]\n",
    "\n",
    "# 删除IsDuplicate列（如果不再需要）\n",
    "df = df.drop(['IsDuplicate'], axis=1)\n",
    "\n",
    "# 创建一个空字典用于存储分子的Bemis-Murcko scaffolds与子集的映射\n",
    "scaffold_to_subset, scaffold_to_index = {}, {}\n",
    "\n",
    "# 获取所有唯一的scaffold类型\n",
    "unique_scaffolds = df['SMILES'].apply(lambda x: Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(x)))).unique()\n",
    "\n",
    "# 将scaffold类型分配到6个子集中\n",
    "num_subsets = 6\n",
    "scaffolds_per_subset = len(unique_scaffolds) // num_subsets\n",
    "remainder = len(unique_scaffolds) % num_subsets\n",
    "\n",
    "# 将唯一的scaffold顺序打乱并分配给子集\n",
    "random.shuffle(unique_scaffolds)\n",
    "\n",
    "scaffold_counter = 0\n",
    "scaffold_index_counter = 0\n",
    "subset_labels = {\n",
    "    1: 'Train #1',\n",
    "    2: 'Train #2',\n",
    "    3: 'Train #3',\n",
    "    4: 'Train #4',\n",
    "    5: 'Train #5',\n",
    "    6: 'Test'\n",
    "}\n",
    "\n",
    "for i in range(1, num_subsets + 1):\n",
    "    num_scaffolds = scaffolds_per_subset\n",
    "    if i <= remainder:\n",
    "        num_scaffolds += 1\n",
    "    subset_scaffolds = unique_scaffolds[scaffold_counter: scaffold_counter + num_scaffolds]\n",
    "    scaffold_counter += num_scaffolds\n",
    "    for scaffold in subset_scaffolds:\n",
    "        scaffold_to_subset[scaffold] = i\n",
    "        scaffold_to_index[scaffold] = scaffold_index_counter\n",
    "        scaffold_index_counter += 1\n",
    "\n",
    "# 将分子分配到子集中\n",
    "df['Subset'] = df['SMILES'].apply(lambda x: subset_labels[scaffold_to_subset.get(Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(x))), 0)])\n",
    "\n",
    "# 在此添加Scanfold Index列\n",
    "df['Scaffold_Index'] = df['SMILES'].apply(lambda x: scaffold_to_index.get(Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(x))), 0))\n",
    "\n",
    "# 在此添加Scaffold_SMILES列\n",
    "df['Scaffold_SMILES'] = df['SMILES'].apply(lambda x: Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(Chem.MolFromSmiles(x))))\n",
    "\n",
    "# 统计每个子集中包含各scaffold索引的数量\n",
    "subset_scaffold_counts = df.groupby(['Subset'])['Scaffold_Index'].nunique().reset_index(name='Scaffold_Count')\n",
    "\n",
    "# 计算每个子集中的总分子数\n",
    "subset_total_counts = df.groupby(['Subset']).size().reset_index(name='Total_Count')\n",
    "\n",
    "# 创建柱状图\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 先获取 \"Test\" 行的数据\n",
    "test_row = subset_total_counts[subset_total_counts['Subset'] == 'Test']\n",
    "# 然后从 subset_total_counts 中删除 \"Test\" 行\n",
    "subset_total_counts = subset_total_counts[subset_total_counts['Subset'] != 'Test']\n",
    "# 将 \"Test\" 行添加到 DataFrame 的末尾\n",
    "subset_total_counts = pd.concat([subset_total_counts, test_row], ignore_index=True)\n",
    "# 先获取 \"Test\" 行的数据\n",
    "test_row = subset_scaffold_counts[subset_scaffold_counts['Subset'] == 'Test']\n",
    "# 然后从 subset_total_counts 中删除 \"Test\" 行\n",
    "subset_scaffold_counts = subset_scaffold_counts[subset_scaffold_counts['Subset'] != 'Test']\n",
    "# 将 \"Test\" 行添加到 DataFrame 的末尾\n",
    "subset_scaffold_counts = pd.concat([subset_scaffold_counts, test_row], ignore_index=True)\n",
    "\n",
    "# 左侧y轴，表示每个子集中的唯一scaffold数量\n",
    "bar1 = ax.bar(subset_total_counts['Subset'], subset_total_counts['Total_Count'], color='skyblue', alpha=0.7, label='Molecules')\n",
    "bar2 = ax.bar(subset_scaffold_counts['Subset'], subset_scaffold_counts['Scaffold_Count'], color='lightcoral', alpha=1, label='Unique Scaffolds')\n",
    "ax.set_xlabel('Data Subset')\n",
    "ax.set_ylabel('Data Number')\n",
    "ax.set_title('Number of Unique Scaffolds and Total Molecules in Each Data Subset')\n",
    "\n",
    "# 使用 subset_labels 字典的值作为 x 轴标签\n",
    "ax.set_xticks(np.arange(0, num_subsets))  # 增加一个位置以容纳 \"Test\"\n",
    "ax.set_xticklabels([subset_labels[i] for i in range(1, num_subsets + 1)])\n",
    "\n",
    "# 图例\n",
    "legend = ax.legend(loc='upper left')#, bbox_to_anchor=(1.01, 0.15)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# 标注柱子上的纵坐标值\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, 5.5*height/7),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bar1)\n",
    "autolabel(bar2)\n",
    "plt.show()\n",
    "\n",
    "# 保存分子子集为不同的CSV文件\n",
    "output_dir = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/'\n",
    "for i in range(1, num_subsets + 1):\n",
    "    subset_df = df[df['Subset'] == subset_labels[i]]\n",
    "    \n",
    "    # 添加Subset列\n",
    "    subset_df['Subset'] = subset_labels[i]\n",
    "\n",
    "    if i == num_subsets:\n",
    "        subset_csv = f'{output_dir}test.csv'\n",
    "    else:\n",
    "        subset_csv = f'{output_dir}train_{i}.csv'\n",
    "    # subset_df.to_csv(subset_csv, index=False) # avoid bad replacement\n",
    "    print(f\"{subset_labels[i]}: {len(subset_df)}\")\n",
    "\n",
    "print(\"分子子集已保存为CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# 定义子集文件路径列表\n",
    "subset_files = [\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_1.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_2.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_3.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_4.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_5.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/test.csv'\n",
    "]\n",
    "\n",
    "# 创建一个空DataFrame来存储提取的数据\n",
    "data = pd.DataFrame(columns=['Subset', 'Scaffold_Index', 'pIC50'])\n",
    "\n",
    "# 定义一个函数来提取IC50值并将其转换为浮点数\n",
    "def extract_ic50(ic50_str):\n",
    "    ic50_match = re.search(r'[<>]?=?\\s*(\\d+(\\.\\d+)?)\\s*[nN][mM]', ic50_str)\n",
    "    if ic50_match:\n",
    "        return float(ic50_match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 遍历子集文件\n",
    "for subset_num, subset_file in enumerate(subset_files, start=1):\n",
    "    subset_df = pd.read_csv(subset_file)\n",
    "    \n",
    "    # 添加Subset列\n",
    "    subset_df['Subset'] = subset_num\n",
    "    \n",
    "    # 添加数据到data DataFrame\n",
    "    data = pd.concat([data, subset_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "subset_labels = {\n",
    "    1: 'Train #1',\n",
    "    2: 'Train #2',\n",
    "    3: 'Train #3',\n",
    "    4: 'Train #4',\n",
    "    5: 'Train #5',\n",
    "    6: 'Test'\n",
    "}\n",
    "# print(data)\n",
    "# 创建二维可视化图\n",
    "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "\n",
    "# 设置Matplotlib字体大小\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "for subset_num, color, marker in zip(range(1, 7), colors, markers):\n",
    "    subset_data = data[data['Subset'] == subset_num]\n",
    "    scaffold_indices = subset_data['Scaffold_Index']\n",
    "    ic50_values = subset_data['pIC50']\n",
    "    ax1.scatter(scaffold_indices + np.random.randn(len(scaffold_indices)) * 0.1, ic50_values, label=f'{subset_labels[subset_num]}', color=color, alpha=0.5, marker=marker)\n",
    "\n",
    "ax1.set_xlabel('Scaffold Index')\n",
    "ax1.set_ylabel('pIC50')\n",
    "ax1.set_title('pIC50 Values for Different Scaffolds in Subsets')\n",
    "legend = ax1.legend(loc='upper center', ncol=3, fontsize=12) # , bbox_to_anchor=(0.5, -0.15)\n",
    "legend.set_title('', prop={'size': 12})\n",
    "\n",
    "# 在右边添加密度曲线\n",
    "ax2 = ax1.inset_axes([1.08, 0, 0.2, 1])\n",
    "ax2.yaxis.tick_right()  # 将第二个y轴的ticks移到右边\n",
    "ax2.yaxis.set_label_position('right')  # 将y轴标签移到右边\n",
    "ax2.set_ylabel('pIC50')\n",
    "ax2.set_xlabel('Density')\n",
    "for subset_num, color in zip(range(1, 7), colors):\n",
    "    subset_data = data[data['Subset'] == subset_num]\n",
    "    ic50_values = subset_data['pIC50']\n",
    "    \n",
    "    # 使用核密度估计计算密度曲线\n",
    "    kde = gaussian_kde(ic50_values)\n",
    "    x_vals = np.linspace(ic50_values.min(), ic50_values.max(), 100)\n",
    "    y_vals = kde(x_vals)\n",
    "    \n",
    "    ax2.plot(y_vals, x_vals, color=color)\n",
    "\n",
    "ax2.set_xlim(0.5, 0)  # 根据数据的范围调整x轴限制\n",
    "ax2.invert_xaxis()   # 反转x轴，使得底边为y轴\n",
    "ax2.grid(True)\n",
    "# ax2.set_title('Density', loc='center')  # 添加密度曲线的标题\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565075f8",
   "metadata": {},
   "source": [
    "## Save SDF for Compounds in Data Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import os\n",
    "\n",
    "activity_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/'\n",
    "target_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/'\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "\n",
    "def save_sdf(activity_path, save_path):\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(activity_path)\n",
    "\n",
    "    # 获取分子SMILES列的数据\n",
    "    smiles_list = df['SMILES'].tolist()\n",
    "    id_list = df['COMPOUND'].tolist()\n",
    "\n",
    "    # 计算并保存SDF文件\n",
    "    for index, smiles in zip(id_list, smiles_list):\n",
    "        output_file = save_path + f'canSAR{index}.sdf'\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            continue\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                print('Cannot read SMILES:', smiles)\n",
    "            hmol = Chem.AddHs(mol)\n",
    "            AllChem.EmbedMolecule(hmol,AllChem.ETKDG())\n",
    "            AllChem.UFFOptimizeMolecule(hmol,1000)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            writer = Chem.SDWriter(output_file)\n",
    "            hmol.SetProp(\"_SMILES\",\"%s\"%smiles)\n",
    "            writer.write(hmol)\n",
    "            writer.close()\n",
    "        except:\n",
    "            print(f'Fail to save SDF for the compound: {index} in the {set_name} set')\n",
    "\n",
    "# 找出每个子集中已保存的SDF文件\n",
    "saved_sdf_files = set()\n",
    "\n",
    "for set_name in tqdm(set_list):\n",
    "    activity_path = f'{activity_root}/{set_name}.csv'\n",
    "    save_path = f'{target_root}/ligand/sdf/{set_name}/'\n",
    "    save_sdf(activity_path, save_path)\n",
    "    \n",
    "    # 获取已保存的SDF文件\n",
    "    saved_sdf_files.update(os.listdir(save_path))\n",
    "\n",
    "# 找到未保存为SDF文件的COMPOUND序号\n",
    "for set_name in set_list:\n",
    "    activity_path = f'{activity_root}/{set_name}.csv'\n",
    "    df = pd.read_csv(activity_path)\n",
    "    \n",
    "    all_compounds = df['COMPOUND'].tolist()\n",
    "    \n",
    "    unsaved_compounds = [compound for compound in all_compounds if f'canSAR{compound}.sdf' not in saved_sdf_files]\n",
    "    \n",
    "    print(f\"未保存为SDF文件的COMPOUND序号 ({set_name}):\")\n",
    "    print(unsaved_compounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb163b",
   "metadata": {},
   "source": [
    "## Save smi list for Compounds in Data Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径列表\n",
    "subset_files = [\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_1.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_2.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_3.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_4.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/train_5.csv',\n",
    "    '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/activity/IC50/test.csv'\n",
    "]\n",
    "\n",
    "# 创建保存 smi 文件的目录\n",
    "output_dir = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/ligand/smi'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 遍历文件并生成 smi 文件\n",
    "for subset_file in subset_files:\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(subset_file)\n",
    "    \n",
    "    # 在 \"COMPOUND\" 列的值前添加 \"canSAR\"\n",
    "    df['COMPOUND'] = 'canSAR' + df['COMPOUND'].astype(str)\n",
    "    \n",
    "    # 提取 \"COMPOUND\" 列和 \"SMILES\" 列的值\n",
    "    compounds = df['COMPOUND'].tolist()\n",
    "    smiles = df['SMILES'].tolist()\n",
    "    \n",
    "    # 创建 smi 文件路径\n",
    "    filename = os.path.join(output_dir, os.path.basename(subset_file).replace('.csv', '.smi'))\n",
    "    \n",
    "    # 写入 smi 文件\n",
    "    with open(filename, 'w') as smi_file:\n",
    "        for compound, smile in zip(compounds, smiles):\n",
    "            smi_file.write(f\"{smile} {compound}\\n\")\n",
    "\n",
    "print(\"SMI 文件已生成并保存在指定目录下\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672c794",
   "metadata": {},
   "source": [
    "## Prepare ligands in pdbqt format for molecular docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60867ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "\n",
    "data_root = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/'\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "\n",
    "input_dir = f'{data_root}/ligand/sdf/'\n",
    "output_dir_addH = f'{data_root}/ligand/addH/'\n",
    "output_dir_pdbqt = f'{data_root}/ligand/pdbqt/'\n",
    "\n",
    "# Loop through input files\n",
    "for set_name in tqdm(set_list):\n",
    "    input_files = os.listdir(input_dir + set_name)\n",
    "    for input_file in tqdm(input_files):\n",
    "        input_file_path = input_dir + set_name + '/' + input_file\n",
    "        # Load the molecule from the SDF file\n",
    "        mol = Chem.SDMolSupplier(input_file_path)[0]\n",
    "\n",
    "        # Add explicit hydrogens\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Save the modified molecule with explicit hydrogens\n",
    "        sdf_addH_save_path = output_dir_addH + set_name + '/'\n",
    "        if not os.path.exists(sdf_addH_save_path):\n",
    "                os.makedirs(sdf_addH_save_path)\n",
    "        Chem.SDWriter(sdf_addH_save_path + input_file).write(mol)\n",
    "\n",
    "        # Construct the command\n",
    "        pdbqt_save_path = output_dir_pdbqt + set_name + '/'\n",
    "        if not os.path.exists(pdbqt_save_path):\n",
    "                os.makedirs(pdbqt_save_path)\n",
    "        output_path = pdbqt_save_path + input_file[:-4] + '.pdbqt'\n",
    "        if not os.path.exists(output_path):\n",
    "            command = f\"mk_prepare_ligand.py -i {input_file_path} -o {output_path}\"\n",
    "            # Execute the command\n",
    "            !{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e16af",
   "metadata": {},
   "source": [
    "## Do AutoDock Vina on a batch of ligands and one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e897e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "protein_name, pdb_name, set_index, pocket_index = 'PARP1', '6nrh', 5, 4\n",
    "# 定义输入输出文件路径列表\n",
    "input_protein = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdbqt'\n",
    "input_dir_ligand = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/ligand/pdbqt/'\n",
    "input_config_dir = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/'\n",
    "output_dir = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/vina/docking_data/'\n",
    "input_sets = [['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5'][set_index]]\n",
    "pocket_sets = [['pocket_1', 'pocket_2', 'pocket_3', 'protein_center', 'pocket_active'][pocket_index]]\n",
    "command_template = \"vina --receptor {} --ligand {} --config {} --exhaustiveness 32 --out {}\"\n",
    "# 循环处理每个文件路径\n",
    "for set_name in tqdm(input_sets):\n",
    "    for pocket_name in pocket_sets:\n",
    "        input_config = input_config_dir + f'vina_{pocket_name}.txt'\n",
    "        input_ligands = os.listdir(input_dir_ligand + set_name)\n",
    "        for input_ligand in tqdm(input_ligands):\n",
    "            # 提取文件名（不包含扩展名）\n",
    "            filename = input_ligand.split('.')[0]\n",
    "            output_path = f'{output_dir}{set_name}/{pocket_name}/{filename}.pdbqt'\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            if os.path.exists(output_path + '.txt'):\n",
    "                os.rename(output_path + '.txt', output_path.replace('pdbqt', 'txt'))\n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "            # 构建命令\n",
    "            command = command_template.format(input_protein, input_dir_ligand + set_name + '/' + input_ligand, input_config, \n",
    "                                              output_path)\n",
    "            # 执行命令并捕获输出结果\n",
    "            output = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "            # 将输出结果保存到文件\n",
    "            with open(output_path.replace('pdbqt', 'txt'), 'w') as f:\n",
    "                f.write(output.stdout)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51c608",
   "metadata": {},
   "source": [
    "## Do QVina-W on a batch of ligands and one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "protein_name, pdb_name, set_index = 'PARP1', '6nrh', 0\n",
    "receptor_file = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdbqt'\n",
    "ligand_pdbqt_dir = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/ligand/pdbqt/'\n",
    "config_file = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/qvina_w.txt'\n",
    "output_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/qvina_w/docking_data/'\n",
    "qvina_w_path = f'/home1/yueming/Drug_Discovery/Baselines/qvina'\n",
    "input_sets = [['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5'][set_index]]\n",
    "# 循环执行命令\n",
    "for set_name in tqdm(input_sets):\n",
    "    output_dir = output_root + set_name + '/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    input_files = os.listdir(ligand_pdbqt_dir + set_name)\n",
    "    for input_file in tqdm(input_files):\n",
    "        if not os.path.exists(f'{output_dir}/{input_file}'):\n",
    "            input_file_path = ligand_pdbqt_dir + set_name + '/' + input_file\n",
    "            with open(config_file, 'r') as f:\n",
    "                config_lines = f.readlines()\n",
    "            config_lines[0] = f'receptor = {receptor_file}\\n'\n",
    "            config_lines[1] = f'ligand = {ligand_pdbqt_dir + set_name}/{input_file}\\n'\n",
    "            config_lines[2] = f'out  = {output_dir}/{input_file}\\n'\n",
    "            config_lines[3] = f'log  = {output_dir}/{input_file}\\n'\n",
    "            with open(config_file, 'w') as f:\n",
    "                f.writelines(config_lines)\n",
    "\n",
    "            # 执行命令\n",
    "            command = f'{qvina_w_path}/qvina-w_serial --config {config_file}'\n",
    "            os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2663a2f",
   "metadata": {},
   "source": [
    "## Do DiffDock on a batch of ligands and one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "protein_name, pdb_name, set_index = 'PARP1', '6nrh', 0\n",
    "receptor_file = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdb'\n",
    "ligand_sdf_dir = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/ligand/sdf/'\n",
    "output_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/diffdock/docking_data/'\n",
    "diffdock_path = f'/home1/yueming/Drug_Discovery/Baselines/DiffDock-main'\n",
    "input_sets = [['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5'][set_index]]\n",
    "# 设置工作目录为包含 inference.py 文件的目录\n",
    "os.chdir(diffdock_path)\n",
    "# 循环执行命令\n",
    "for set_name in tqdm(input_sets):\n",
    "    output_dir = output_root + set_name + '/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    input_files = os.listdir(ligand_sdf_dir + set_name)\n",
    "    for input_file in tqdm(input_files):\n",
    "        filename = input_file.split('.')[0]\n",
    "        if not os.path.exists(f'{output_dir}/{filename}/rank1.sdf'):\n",
    "            input_file_path = ligand_sdf_dir + set_name + '/' + input_file\n",
    "            # 执行命令\n",
    "            command = f'python {diffdock_path}/inference.py --complex_name {filename} --protein_path {receptor_file} --ligand {input_file_path} --out_dir {output_dir} --inference_steps 20 --samples_per_complex 10 --batch_size 10 --actual_steps 18 --no_final_step_noise'\n",
    "            os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907dd093",
   "metadata": {},
   "source": [
    "## Do GNINA on a batch of ligands and one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /home1/yueming/Drug_Discovery/Baselines/gnina\n",
    "# sudo docker run -v /home1:$(pwd)/home1 -it gnina/gnina\n",
    "# /home1/yueming/Drug_Discovery/Baselines/gnina/home1/yueming/anaconda3/bin/python /home1/yueming/Drug_Discovery/Baselines/gnina/home1/yueming/Drug_Discovery/Baselines/gnina/run_gnina.py\n",
    "# sudo chown -R yueming:yueming /home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/6nrh/gnina\n",
    "# rm -r /home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/PARP1/6nrh/gnina/test\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "protein_name, pdb_name, set_index = 'PARP1', '6nrh', 0\n",
    "gnina_path = f'/home1/yueming/Drug_Discovery/Baselines/gnina'\n",
    "data_root = gnina_path + f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}'\n",
    "receptor_file = gnina_path + f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdb'\n",
    "ligand_pdbqt_dir = gnina_path + f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/ligand/sdf/'\n",
    "output_root = gnina_path + f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/gnina/docking_data/'\n",
    "input_sets = [['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5'][set_index]]\n",
    "# 设置工作目录为包含 inference.py 文件的目录\n",
    "os.chdir(gnina_path)\n",
    "# 循环执行命令\n",
    "for set_name in tqdm(input_sets):\n",
    "    output_dir = output_root + set_name + '/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    input_files = os.listdir(ligand_pdbqt_dir + set_name)\n",
    "    for input_file in tqdm(input_files):\n",
    "        filename = input_file.split('.')[0]\n",
    "        if not os.path.exists(f'{output_dir}{filename}.pdbqt'):\n",
    "            input_file_path = ligand_pdbqt_dir + set_name + '/' + input_file\n",
    "            # 执行命令\n",
    "            command = f'gnina --out {output_dir}{filename}.pdbqt --receptor {receptor_file} --ligand {input_file_path} --center_x 22.945 --center_y -10.253 --center_z 14.962 --size_x 18.0 --size_y 27.0 --size_z 16.5 --device 0'\n",
    "            os.system(command)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375e26a",
   "metadata": {},
   "source": [
    "## Do KarmaDock on a batch of ligands and one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "protein_name, pdb_name = 'PARP1', '6nrh'\n",
    "receptor_file = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdb'\n",
    "ligand_smi_dir = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/ligand/smi/'\n",
    "output_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/karmadock/docking_data/'\n",
    "karmadock_path = '/home1/yueming/Drug_Discovery/Baselines/KarmaDock-main/'\n",
    "input_sets = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "pocket_center_dict = {'pocket_active': [22.945, -10.253, 14.962], 'pocket_1': [26.375,-8.04,9.11], 'pocket_2': [38.309,4.975,18.243], 'pocket_3': [29.747,-8.243,32.118], 'protein_center': [25.61,-4.054,18.329]}\n",
    "# 设置工作目录为包含 inference.py 文件的目录\n",
    "os.chdir(karmadock_path + 'utils')\n",
    "# 循环执行命令\n",
    "for set_name in tqdm(input_sets):\n",
    "    output_dir = output_root + set_name + '/'\n",
    "    input_file = ligand_smi_dir + f'{set_name}.smi'\n",
    "    for key, value in pocket_center_dict.items():\n",
    "        output_path = output_dir + key\n",
    "        if not os.path.exists(f'{output_path}/1'):\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            # 执行命令\n",
    "            command = f'python -u virtual_screening.py --ligand_smi {input_file} --protein_file {receptor_file} --crystal_ligand_file \"{value}\" --model_file ../trained_models/karmadock_screening.pkl --out_dir {output_path} --batch_size 64 --random_seed 2023'\n",
    "            print(command)\n",
    "            os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dd1ac",
   "metadata": {},
   "source": [
    "## Preprocessing docking results to mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d4e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda activate rdkit\n",
    "import os, re\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymol\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "def generate_pocket(dataset_path_template, dock_software_list, input_ligand_format_dict, protein_path, distance=5, save_states=1, generated_data_folder_name='pocket_complex', reverse=False):\n",
    "    for dock_software in dock_software_list:\n",
    "        print(f'Generating pockets for {dock_software} results')\n",
    "        input_ligand_format = input_ligand_format_dict[dock_software]\n",
    "        data_dir = dataset_path_template.format(dock_software, 'docking_data')\n",
    "        save_dir = dataset_path_template.format(dock_software, generated_data_folder_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        complex_id = os.listdir(data_dir)\n",
    "        complex_id.sort(reverse=reverse)\n",
    "        for file in complex_id:\n",
    "            lig_path_list, pocket_save_path_list = [], []\n",
    "            if os.path.isdir(os.path.join(data_dir, file)):\n",
    "                cid = file\n",
    "                if dock_software == 'diffdock':\n",
    "                    ranking_files = os.listdir(os.path.join(data_dir, cid))\n",
    "                    pattern = r'rank(\\d+)_'\n",
    "                    for ranking_file in ranking_files:\n",
    "                        match = re.search(pattern, ranking_file)\n",
    "                        if match:\n",
    "                            matched_number = match.group(1)\n",
    "                            rank = int(matched_number)\n",
    "                            if rank <= save_states:\n",
    "                                lig_path_list.append(os.path.join(data_dir, cid, ranking_file))\n",
    "                                pocket_save_path_list.append(os.path.join(save_dir, cid, f'{cid}_{dock_software}_rank{rank}_pocket_{distance}A.pdb'))\n",
    "                elif dock_software == 'tankbind': # just one state\n",
    "                    p2rank_pocket_file_list = os.listdir(os.path.join(data_dir, cid))\n",
    "                    p2rank_pocket_name_list = [p2rank_pocket_file.split('.')[0] for p2rank_pocket_file in p2rank_pocket_file_list]\n",
    "                    lig_path_list = [os.path.join(data_dir, cid, p2rank_pocket_file) for p2rank_pocket_file in p2rank_pocket_file_list]\n",
    "                    pocket_save_path_list = [os.path.join(save_dir, p2rank_pocket_name, cid, f'{cid}_{dock_software}_{p2rank_pocket_name}_pocket_{distance}A.pdb')for p2rank_pocket_name in p2rank_pocket_name_list]\n",
    "                elif dock_software == 'karmadock':\n",
    "                    pocket_name = file\n",
    "                    file_path_list = os.listdir(os.path.join(data_dir, pocket_name))\n",
    "                    cid_list = [_.split('_')[0] for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    suffix_list = [_.split('_', 1)[-1].split('.')[0] for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    lig_path_list = [os.path.join(data_dir, pocket_name, _) for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    pocket_save_path_list = []\n",
    "                    for cid, suffix in zip(cid_list, suffix_list):\n",
    "                        pocket_save_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_{suffix}_pocket_{distance}A.pdb'))\n",
    "                elif dock_software == 'vina':\n",
    "                    pocket_name = file\n",
    "                    file_path_list = os.listdir(os.path.join(data_dir, pocket_name))\n",
    "                    for ligand_result_file in file_path_list:\n",
    "                        file_suffix = ligand_result_file.split('.')[-1]\n",
    "                        if file_suffix == input_ligand_format:\n",
    "                            cid = ligand_result_file.split('.')[0]\n",
    "                            for save_state in range(save_states):\n",
    "                                lig_path_list.append(os.path.join(data_dir, pocket_name, ligand_result_file))\n",
    "                                pocket_save_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_pose{save_state}_pocket_{distance}A.pdb'))\n",
    "            else:\n",
    "                if file.split('.')[-1] == input_ligand_format:\n",
    "                    cid = file.split('.')[0]\n",
    "                    lig_path_list = [os.path.join(data_dir, file) for save_state in range(save_states)]\n",
    "                    pocket_save_path_list = [os.path.join(save_dir, cid, f'{cid}_{dock_software}_pose{save_state+1}_pocket_{distance}A.pdb') for save_state in range(save_states)]\n",
    "\n",
    "            protein_name = protein_path.split('/')[-1].split('.')[0]\n",
    "            for lig_path, pocket_path in zip(lig_path_list, pocket_save_path_list):\n",
    "                if not os.path.exists(pocket_path) or recreate:\n",
    "                    pymol.cmd.delete('all')\n",
    "                    pymol.cmd.load(lig_path)\n",
    "                    pymol.cmd.remove('hydrogens')\n",
    "                    pymol.cmd.load(protein_path)\n",
    "                    pymol.cmd.remove('resn HOH')\n",
    "                    object_list = pymol.cmd.get_object_list()  # 获取所有对象列表\n",
    "                    try:\n",
    "                        obj_ligand, obj_protein = object_list[0], object_list[1]\n",
    "                    except:\n",
    "                        print(f'No docking data found for {lig_path}') # if no molecule in file\n",
    "                        continue\n",
    "                    pattern = r'_pose(\\d+)_'\n",
    "                    match = re.search(pattern, pocket_path)\n",
    "                    if match:\n",
    "                        matched_number = match.group(1)\n",
    "                        state = int(matched_number)\n",
    "                    else:\n",
    "                        state = 1\n",
    "                \n",
    "                    pymol.cmd.create(f\"state_{state}\", obj_ligand, source_state=state, target_state=1) # target_state is the state of the new created ones\n",
    "                    pymol.cmd.select('Pocket', f'byres {protein_name} within {distance} of state_{state}')\n",
    "                    os.makedirs(os.path.dirname(pocket_path), exist_ok=True)\n",
    "                    pymol.cmd.save(pocket_path, 'Pocket')\n",
    "\n",
    "\n",
    "def split_ligand_to_pdb(ligand_input_path, lig_save_path, save_state=1, generated_data_folder_name='pocket_complex'):\n",
    "    pymol.cmd.delete('all')\n",
    "    pdb_name = ligand_input_path.split('/')[-1].split('.')[0]\n",
    "    pymol.cmd.load(ligand_input_path)\n",
    "    pymol.cmd.remove('hydrogens')\n",
    "    object_list = pymol.cmd.get_object_list()  # 获取所有对象列表\n",
    "    try:\n",
    "        obj_ligand = object_list[0]\n",
    "    except:\n",
    "        print(f'No molecule found in: {ligand_input_path}')\n",
    "        return 0\n",
    "    total_states = pymol.cmd.count_states(obj_ligand)\n",
    "    if save_state <= total_states:\n",
    "        save_path = lig_save_path.rsplit('.', 1)[0] + f'_pose{save_state}.pdb'\n",
    "        save_path = save_path.replace('docking_data', generated_data_folder_name)\n",
    "        if not os.path.exists(save_path) or recreate:\n",
    "            pymol.cmd.create(f\"state_{save_state}\", obj_ligand, save_state, 1)\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            pymol.cmd.save(save_path, f\"state_{save_state}\")\n",
    "    else:\n",
    "        print(f'save_state {save_state} is greater than the total_states {total_states}')\n",
    "                    \n",
    "                \n",
    "def generate_complex(dataset_path_template, dock_software_list, input_ligand_format_dict, distance=5, save_states=1, generated_data_folder_name='pocket_complex', reverse=False):\n",
    "    for dock_software in dock_software_list:\n",
    "        print(f'Generating complexes for {dock_software} results')\n",
    "        input_ligand_format = input_ligand_format_dict[dock_software]\n",
    "        data_dir = dataset_path_template.format(dock_software, 'docking_data')\n",
    "        save_dir = dataset_path_template.format(dock_software, generated_data_folder_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_list = os.listdir(data_dir)\n",
    "        file_list.sort(reverse=reverse)\n",
    "        for file in file_list:\n",
    "            lig_path_list, lig_save_path_list, pocket_path_list, complex_save_path_list = [], [], [], []\n",
    "            if os.path.isdir(os.path.join(data_dir, file)):\n",
    "                cid = file\n",
    "                if dock_software == 'diffdock':\n",
    "                    ranking_files = os.listdir(os.path.join(data_dir, cid))\n",
    "                    pattern = r'rank(\\d+)_'\n",
    "                    for ranking_file in ranking_files:\n",
    "                        match = re.search(pattern, ranking_file)\n",
    "                        if match:\n",
    "                            matched_number = match.group(1)\n",
    "                            rank = int(matched_number)\n",
    "                            if rank <= save_states:\n",
    "                                lig_path_list.append(os.path.join(data_dir, cid, ranking_file))\n",
    "                                pocket_path_list.append(os.path.join(save_dir, cid, f'{cid}_{dock_software}_rank{rank}_pocket_{distance}A.pdb'))\n",
    "                                complex_save_path_list.append(os.path.join(save_dir, cid, f'{cid}_{dock_software}_rank{rank}_complex_{distance}A.rdkit'))\n",
    "                    lig_save_path_list = lig_path_list\n",
    "                elif dock_software == 'tankbind': # just one state\n",
    "                    p2rank_pocket_file_list = os.listdir(os.path.join(data_dir, cid))\n",
    "                    p2rank_pocket_name_list = [p2rank_pocket_file.split('.')[0] for p2rank_pocket_file in p2rank_pocket_file_list]\n",
    "                    lig_path_list = [os.path.join(data_dir, cid, p2rank_pocket_file) for p2rank_pocket_file in p2rank_pocket_file_list]\n",
    "                    lig_save_path_list = [os.path.join(save_dir, p2rank_pocket_name, cid, p2rank_pocket_name) for p2rank_pocket_name in p2rank_pocket_name_list]\n",
    "                    pocket_path_list = [os.path.join(save_dir, p2rank_pocket_name, cid, f'{cid}_{dock_software}_{p2rank_pocket_name}_pocket_{distance}A.pdb')for p2rank_pocket_name in p2rank_pocket_name_list]\n",
    "                    complex_save_path_list = [os.path.join(save_dir, p2rank_pocket_name, cid, f'{cid}_{dock_software}_{p2rank_pocket_name}_complex_{distance}A.rdkit')for p2rank_pocket_name in p2rank_pocket_name_list]\n",
    "                elif dock_software == 'karmadock':\n",
    "                    pocket_name = file\n",
    "                    file_path_list = os.listdir(os.path.join(data_dir, pocket_name))\n",
    "                    cid_list = [_.split('_')[0] for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    suffix_list = [_.split('_', 1)[-1].split('.')[0] for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    lig_path_list = [os.path.join(data_dir, pocket_name, _) for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    lig_save_path_list = [os.path.join(save_dir, pocket_name, _.split('_')[0], _) for _ in file_path_list if _.split('.')[-1]==input_ligand_format]\n",
    "                    for cid, suffix in zip(cid_list, suffix_list):\n",
    "                        pocket_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_{suffix}_pocket_{distance}A.pdb'))\n",
    "                        complex_save_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_{suffix}_complex_{distance}A.rdkit'))      \n",
    "                elif dock_software == 'vina':\n",
    "                    pocket_name = file\n",
    "                    file_path_list = os.listdir(os.path.join(data_dir, pocket_name))\n",
    "                    for ligand_result_file in file_path_list:\n",
    "                        file_suffix = ligand_result_file.split('.')[-1]\n",
    "                        if file_suffix == input_ligand_format:\n",
    "                            cid = ligand_result_file.split('.')[0]\n",
    "                            for save_state in range(save_states):\n",
    "                                lig_path_list.append(os.path.join(data_dir, pocket_name, ligand_result_file))\n",
    "                                lig_save_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_pose{save_state}.{input_ligand_format}'))\n",
    "                                pocket_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_pose{save_state}_pocket_{distance}A.pdb'))\n",
    "                                complex_save_path_list.append(os.path.join(save_dir, pocket_name, cid, f'{cid}_{dock_software}_{pocket_name}_pose{save_state}_complex_{distance}A.rdkit'))      \n",
    "            else:\n",
    "                if file.split('.')[-1] == input_ligand_format:\n",
    "                    cid = file.split('.')[0]\n",
    "                    lig_path_list = [os.path.join(data_dir, file) for save_state in range(save_states)]\n",
    "                    lig_save_path_list = [os.path.join(save_dir, cid, file) for save_state in range(save_states)]\n",
    "                    pocket_path_list = [os.path.join(save_dir, cid, f'{cid}_{dock_software}_pose{save_state+1}_pocket_{distance}A.pdb') for save_state in range(save_states)]\n",
    "                    complex_save_path_list = [os.path.join(save_dir, cid, f'{cid}_{dock_software}_pose{save_state+1}_complex_{distance}A.rdkit') for save_state in range(save_states)]\n",
    "\n",
    "            for ligand_input_path, lig_save_path, pocket_path, complex_save_path in zip(lig_path_list, lig_save_path_list, pocket_path_list, complex_save_path_list):\n",
    "                if not os.path.exists(complex_save_path) or recreate:\n",
    "                    pattern = r'_pose(\\d+)_'\n",
    "                    match = re.search(pattern, pocket_path)\n",
    "                    if match:\n",
    "                        matched_number = match.group(1)\n",
    "                        pose = int(matched_number)\n",
    "                    else:\n",
    "                        pose = 1\n",
    "                    split_ligand_to_pdb(ligand_input_path, lig_save_path, save_state=pose, generated_data_folder_name=generated_data_folder_name)\n",
    "\n",
    "                    ligand_path = lig_save_path.rsplit('.', 1)[0] + f'_pose{pose}.pdb' # one state of the ligand\n",
    "                    ligand_path = ligand_path.replace('docking_data', generated_data_folder_name) # where the \"split_ligand_to_pdb\" function save the ligand states\n",
    "                    if not os.path.exists(ligand_path):\n",
    "                        print(f'Not found pose: {ligand_path}')\n",
    "                        continue\n",
    "\n",
    "                    ligand_file = Chem.MolFromPDBFile(ligand_path, removeHs=True)\n",
    "                    if ligand_file == None:\n",
    "                        print(f\"Unable to process ligand: {ligand_path}\")\n",
    "                        continue\n",
    "\n",
    "                    pocket_file = Chem.MolFromPDBFile(pocket_path, removeHs=True)\n",
    "                    if pocket_file == None:\n",
    "                        print(f\"Unable to process pocket: {pocket_path}\")\n",
    "                        continue\n",
    "\n",
    "                    complex = (ligand_file, pocket_file)\n",
    "                    os.makedirs(os.path.dirname(complex_save_path), exist_ok=True)\n",
    "                    with open(complex_save_path, 'wb') as f:\n",
    "                        pickle.dump(complex, f)\n",
    "    \n",
    "    \n",
    "generated_data_folder_name, recreate, reverse = 'pocket_complex', False, False\n",
    "protein_name, assay_type, pdb_name, pose_num, distance = 'PARP1', 'IC50', '6nrh', 9, 5\n",
    "dock_software_list = [['karmadock', 'diffdock', 'tankbind', 'gnina', 'qvina_w', 'vina'][5]] # qvina_w need to generate subset by subset, in avoid of corrupt\n",
    "input_ligand_format_dict = {'karmadock': 'sdf', 'diffdock': 'sdf', 'tankbind': 'sdf', 'gnina': 'pdbqt', 'qvina_w': 'pdbqt', 'vina': 'pdbqt'}\n",
    "input_protein_format_dict = {'karmadock': 'pdb', 'diffdock': 'pdb', 'tankbind': 'pdb', 'gnina': 'pdb', 'qvina_w': 'pdbqt', 'vina': 'pdbqt'}\n",
    "protein_path = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/protein/{pdb_name}_protein.pdbqt'\n",
    "data_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/'\n",
    "activity_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/activity'\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "for dataset in tqdm(set_list):\n",
    "    print(f'Processing {dataset} ...')\n",
    "    data_df = pd.read_csv(os.path.join(activity_root, assay_type, f'{dataset}.csv'))\n",
    "    dataset_path_template = data_root + '{}/{}/' + dataset + '/'\n",
    "\n",
    "    ## generate pocket within 12 Ångström around ligand \n",
    "    generate_pocket(dataset_path_template=dataset_path_template, dock_software_list=dock_software_list, input_ligand_format_dict=input_ligand_format_dict, protein_path=protein_path, distance=distance, save_states=pose_num, generated_data_folder_name=generated_data_folder_name, reverse=reverse)\n",
    "    generate_complex(dataset_path_template=dataset_path_template, dock_software_list=dock_software_list, input_ligand_format_dict=input_ligand_format_dict, distance=distance, save_states=pose_num, generated_data_folder_name=generated_data_folder_name, reverse=reverse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c05f4",
   "metadata": {},
   "source": [
    "## Save DTIGN graphs for each ligand with bond features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdcfb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda activate base\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.spatial import distance_matrix\n",
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "import networkx as nx\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch, Data\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "# %%\n",
    "def one_of_k_encoding(k, possible_values):\n",
    "    if k not in possible_values:\n",
    "        raise ValueError(f\"{k} is not a valid value in {possible_values}\")\n",
    "    return [k == e for e in possible_values]\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def atom_features(mol, graph, atom_symbols=['C', 'N', 'O', 'S', 'F', 'P', 'Cl', 'Br', 'I'], explicit_H=True):\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        results = one_of_k_encoding_unk(atom.GetSymbol(), atom_symbols + ['Unknown']) + \\\n",
    "                one_of_k_encoding_unk(atom.GetDegree(),[0, 1, 2, 3, 4, 5, 6]) + \\\n",
    "                one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n",
    "                one_of_k_encoding_unk(atom.GetHybridization(), [\n",
    "                    Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                    Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n",
    "                                        SP3D, Chem.rdchem.HybridizationType.SP3D2\n",
    "                    ]) + [atom.GetIsAromatic()]\n",
    "        # In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`\n",
    "        if explicit_H:\n",
    "            results = results + one_of_k_encoding_unk(atom.GetTotalNumHs(),\n",
    "                                                    [0, 1, 2, 3, 4])\n",
    "\n",
    "        atom_feats = np.array(results).astype(np.float32)\n",
    "\n",
    "        graph.add_node(atom.GetIdx(), feats=torch.from_numpy(atom_feats))\n",
    "\n",
    "def bond_features(bond, use_chirality=True):\n",
    "    bt = bond.GetBondType()\n",
    "    bond_feats = [\n",
    "        bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE,\n",
    "        bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC,\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing()\n",
    "    ]\n",
    "    if use_chirality:\n",
    "        bond_feats = bond_feats + one_of_k_encoding_unk(\n",
    "            str(bond.GetStereo()),\n",
    "            [\"STEREONONE\", \"STEREOANY\", \"STEREOZ\", \"STEREOE\"])\n",
    "    return np.array(bond_feats)\n",
    "        \n",
    "def get_edge_index(mol, graph):\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_feats = bond_features(bond)\n",
    "        graph.add_edge(i, j, weight=bond_feats)\n",
    "\n",
    "def mol2graph(mol):\n",
    "    graph = nx.Graph()\n",
    "    atom_features(mol, graph)\n",
    "    get_edge_index(mol, graph)\n",
    "\n",
    "    graph = graph.to_directed()\n",
    "    x = torch.stack([feats['feats'] for n, feats in graph.nodes(data=True)])\n",
    "    x_bond = torch.tensor([graph[u][v]['weight'] for u, v in graph.edges()], dtype=torch.float32)\n",
    "    if not graph.edges(data=False):\n",
    "        return [], [], [], True\n",
    "    edge_index = torch.stack([torch.LongTensor((u, v)) for u, v in graph.edges(data=False)]).T\n",
    "\n",
    "    return x, x_bond, edge_index, False\n",
    "\n",
    "def inter_graph(ligand, pocket, dis_threshold = 5.):\n",
    "    atom_num_l = ligand.GetNumAtoms()\n",
    "    atom_num_p = pocket.GetNumAtoms()\n",
    "\n",
    "    graph_inter = nx.Graph()\n",
    "    pos_l = ligand.GetConformers()[0].GetPositions()\n",
    "    pos_p = pocket.GetConformers()[0].GetPositions()\n",
    "    dis_matrix = distance_matrix(pos_l, pos_p)\n",
    "    node_idx = np.where(dis_matrix < dis_threshold)\n",
    "    for i, j in zip(node_idx[0], node_idx[1]):\n",
    "        graph_inter.add_edge(i, j+atom_num_l) \n",
    "\n",
    "    graph_inter = graph_inter.to_directed()\n",
    "    edge_index_inter = torch.stack([torch.LongTensor((u, v)) for u, v in graph_inter.edges(data=False)]).T\n",
    "\n",
    "    return edge_index_inter\n",
    "\n",
    "# %%\n",
    "def mols2graphs(complex_path_list, label, save_path, dis_threshold):\n",
    "    data_list = []\n",
    "    fail_path = []\n",
    "    for i, complex_path in enumerate(complex_path_list):\n",
    "        if os.path.exists(complex_path):\n",
    "            try:\n",
    "                with open(complex_path, 'rb') as f:\n",
    "                    ligand, pocket = pickle.load(f)\n",
    "            except EOFError:\n",
    "                print(f'Error: Ran out of input when unpickling. Check the file contents: {complex_path}')\n",
    "                continue\n",
    "        else:\n",
    "            print('Complex file not found:', complex_path)\n",
    "            fail_path.append(complex_path)\n",
    "            continue\n",
    "\n",
    "        atom_num_l = ligand.GetNumAtoms()\n",
    "        atom_num_p = pocket.GetNumAtoms()\n",
    "\n",
    "        pos_l = torch.FloatTensor(ligand.GetConformers()[0].GetPositions())\n",
    "        pos_p = torch.FloatTensor(pocket.GetConformers()[0].GetPositions())\n",
    "        x_l, x_l_bond, edge_index_l, fail_l = mol2graph(ligand)\n",
    "        x_p, x_p_bond, edge_index_p, fail_p = mol2graph(pocket)\n",
    "        if fail_l or fail_p:\n",
    "            print('Failed to read complex file:', complex_path)\n",
    "            fail_path.append(complex_path)\n",
    "            continue\n",
    "\n",
    "        x = torch.cat([x_l, x_p], dim=0)\n",
    "        x_bond = torch.cat([x_l_bond, x_p_bond], dim=0)\n",
    "        edge_index_intra = torch.cat([edge_index_l, edge_index_p+atom_num_l], dim=-1)\n",
    "        try:\n",
    "            edge_index_inter = inter_graph(ligand, pocket, dis_threshold=dis_threshold)\n",
    "        except:\n",
    "            print('Failed to read complex edges:', complex_path)\n",
    "            fail_path.append(complex_path)\n",
    "            continue\n",
    "            \n",
    "        y = torch.FloatTensor([label])\n",
    "        pos = torch.concat([pos_l, pos_p], dim=0)\n",
    "        split = torch.cat([torch.zeros((atom_num_l, )), torch.ones((atom_num_p,))], dim=0)\n",
    "        dock_software = complex_path.split('/')[-1].split('_')[1]\n",
    "        pattern = f\"{dock_software}_(.*?)_complex\"\n",
    "        match = re.search(pattern, complex_path)\n",
    "        pocket_or_pose = ''\n",
    "        if match:\n",
    "            pocket_or_pose = match.group(1)\n",
    "        \n",
    "        data = Data(x=x, x_bond=x_bond, edge_index_intra=edge_index_intra, edge_index_inter=edge_index_inter, y=y, pos=pos, dock_software=dock_software, pocket_or_pose=pocket_or_pose, split=split)\n",
    "        data_list.append(data)\n",
    "        \n",
    "    if len(fail_path) == len(complex_path_list):\n",
    "        return complex_path_list\n",
    "    else:\n",
    "        merged_data = Batch.from_data_list(data_list)\n",
    "        if not (skip_exist and os.path.exists(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save(merged_data, save_path)\n",
    "        return fail_path\n",
    "\n",
    "# %%\n",
    "class PLIDataLoader(DataLoader):\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(data, collate_fn=data.collate_fn, **kwargs)\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is used for generating graph objects using multi process\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path_template, data_df, dock_software_list, save_dir, run_datafold=None, dis_threshold=5, num_pose=1, graph_type='Graph_GIGN', assay_type='pIC50', num_process=8, create=False, addition=False):\n",
    "        self.dataset_path_template = dataset_path_template\n",
    "        self.dock_software_list = dock_software_list\n",
    "        self.save_dir = save_dir\n",
    "        self.data_df = data_df\n",
    "        self.dis_threshold = dis_threshold\n",
    "        self.num_pose = num_pose\n",
    "        self.graph_type = graph_type\n",
    "        self.create = create\n",
    "        self.graph_paths = None\n",
    "        self.compound_ids = None\n",
    "        self.assay_type = assay_type\n",
    "        self.num_process = num_process\n",
    "        self.mean, self.std = 0, 1\n",
    "        self.run_datafold = run_datafold\n",
    "        self.addition = addition\n",
    "        self._pre_process()\n",
    "\n",
    "    def _pre_process(self):\n",
    "        dataset_path_template = self.dataset_path_template\n",
    "        dock_software_list = self.dock_software_list\n",
    "        data_df = self.data_df\n",
    "        graph_type = self.graph_type\n",
    "        save_dir = self.save_dir\n",
    "\n",
    "        complex_path_list, compound_id_list, pIC50_list, score_list, graph_path_list, dis_threshold_list = [], [], [], [], [], []\n",
    "        not_found_list, not_found_flag = [], True\n",
    "        for i, row in data_df.iterrows():\n",
    "            cid, pIC50 = 'canSAR' + str(row['COMPOUND']), float(row['p' + self.assay_type]) # e.g. pIC50\n",
    "            complex_path_list_cid, compound_id_list_cid, score_list_cid = [], [], []\n",
    "            for dock_software in dock_software_list:\n",
    "                data_dir = dataset_path_template.format(dock_software, 'pocket_complex')\n",
    "                file_list = os.listdir(data_dir)\n",
    "                if dock_software in ['karmadock', 'tankbind', 'vina']: # cid files in pocket folders\n",
    "                    for pocket in file_list:\n",
    "                        cid_folder = os.path.join(data_dir, pocket, cid)\n",
    "                        if os.path.exists(cid_folder):\n",
    "                            cid_folder_files = os.listdir(cid_folder)\n",
    "                            rdkit_file_list = [f'{cid}_vina_{pocket}_pose{pose}_complex_{self.dis_threshold}A.rdkit' for pose in range(self.num_pose)] if dock_software == 'vina' else [file for file in cid_folder_files if file.split('.')[-1]=='rdkit']\n",
    "                            if rdkit_file_list:\n",
    "                                for rdkit_file in rdkit_file_list:\n",
    "                                    complex_path = os.path.join(data_dir, pocket, cid, rdkit_file)\n",
    "                                    if os.path.exists(complex_path):\n",
    "                                        not_found_flag = False\n",
    "                                        complex_path_list_cid.append(complex_path)\n",
    "                else:\n",
    "                    cid_folder = os.path.join(data_dir, cid)\n",
    "                    if os.path.exists(cid_folder):\n",
    "                        cid_folder_files = os.listdir(cid_folder)\n",
    "                        rdkit_file_list = [file for file in cid_folder_files if file.split('.')[-1]=='rdkit']\n",
    "                        if rdkit_file_list:\n",
    "                            not_found_flag = False\n",
    "                            for rdkit_file in rdkit_file_list:\n",
    "                                complex_path = os.path.join(data_dir, cid, rdkit_file)\n",
    "                                complex_path_list_cid.append(complex_path)\n",
    "            \n",
    "            graph_path = os.path.join(save_dir, f\"{cid}_{graph_type}_{self.dis_threshold}A.pyg\")\n",
    "            if not_found_flag:\n",
    "                not_found_list.append(graph_path)\n",
    "            else:\n",
    "                graph_path_list.append(graph_path)\n",
    "                complex_path_list.append(complex_path_list_cid)\n",
    "                compound_id_list.append(cid)\n",
    "                pIC50_list.append(pIC50)\n",
    "                dis_threshold_list.append(self.dis_threshold)\n",
    "                \n",
    "        self.mean, self.std = np.mean(pIC50_list), np.std(pIC50_list)\n",
    "        if self.create:\n",
    "            print('Generate complex graph...')\n",
    "            # multi-thread processing\n",
    "            pool = multiprocessing.Pool(self.num_process)\n",
    "            for complex_path, pIC50 ,graph_path, dis_threshold in tqdm(zip(complex_path_list, pIC50_list, graph_path_list, dis_threshold_list)):\n",
    "                not_found_path = mols2graphs(complex_path, pIC50 ,graph_path, dis_threshold)\n",
    "                if len(not_found_path) == len(complex_path):\n",
    "                    not_found_list.append(graph_path)\n",
    "            not_found_save_path = save_dir + f'/{self.graph_type}_not_found_list.pkl'\n",
    "            os.makedirs(os.path.dirname(not_found_save_path), exist_ok=True)\n",
    "            if self.addition:\n",
    "                print(f'Adding not found graph paths into {not_found_save_path}...')\n",
    "                with open(not_found_save_path, 'rb') as f:\n",
    "                    existing_not_found_list = pickle.load(f)\n",
    "                not_found_list = existing_not_found_list + not_found_list\n",
    "            with open(not_found_save_path, 'wb') as f:\n",
    "                pickle.dump(not_found_list, f)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        \n",
    "        with open(save_dir + f'/{self.graph_type}_not_found_list.pkl', 'rb') as f:\n",
    "            not_found_list = pickle.load(f)\n",
    "        not_found_list = [path.replace(self.run_datafold, '.') for path in not_found_list]\n",
    "        graph_path_list = [path.replace(self.run_datafold, '.') for path in graph_path_list]\n",
    "        self.compound_ids = compound_id_list\n",
    "        self.graph_paths = [x for x in graph_path_list if x not in not_found_list] # excluding unsuccessful creation\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.graph_paths[idx])\n",
    "        match = re.search(r'canSAR(\\d+)_', self.graph_paths[idx])\n",
    "        cid = match.group(0)\n",
    "        data['idx'] = cid[:-1]\n",
    "        return data\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return Batch.from_data_list(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_paths)\n",
    "\n",
    "    \n",
    "skip_exist, graph_type = True, 'Graph_DTIGN'\n",
    "protein_name, assay_type, pdb_name, pose_num, dis_threshold = 'PARP1', 'IC50', '6nrh', 3, 5\n",
    "dock_software_list = [['karmadock', 'diffdock', 'tankbind', 'gnina', 'qvina_w', 'vina'][5]] # qvina_w need to generate subset by subset, in avoid of corrupt\n",
    "data_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/{pdb_name}/'\n",
    "activity_root = f'/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/data/{protein_name}/activity'\n",
    "run_datafold = '/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN'\n",
    "set_list = ['test', 'train_1', 'train_2', 'train_3', 'train_4', 'train_5']\n",
    "dock_software_string = '_'.join(dock_software_list)\n",
    "generated_data_folder_name = f'{graph_type}_{dock_software_string}_pose{pose_num}_{dis_threshold}A'\n",
    "for set_name in set_list:\n",
    "    data_df = pd.read_csv(os.path.join(activity_root, assay_type, f'{set_name}.csv'))\n",
    "    dataset_path_template = data_root + '{}/{}/' + set_name + '/'\n",
    "    save_dir = data_root + f'{generated_data_folder_name}/' + set_name + '/'\n",
    "    dataset = GraphDataset(dataset_path_template, data_df, dock_software_list, save_dir, run_datafold=run_datafold, graph_type=graph_type, assay_type=assay_type, dis_threshold=dis_threshold, create=True)\n",
    "    print('Dataset size:', len(dataset))\n",
    "    data_loader = PLIDataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "    for data in data_loader:\n",
    "        # print(data) --> DataBatch(x=[2481, 35], y=[8], pos=[2481, 3], edge_index_intra=[2, 4884], edge_index_inter=[2, 4456], split=[2481], pocket=[8], batch=[2481], ptr=[9])\n",
    "        data, pocket, idx, label, software = data, data.pocket_or_pose, data.idx, data.y, data.dock_software\n",
    "        print(f'Loading {len(pocket)} data successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c9bd6-1723-4296-a345-db95af82401b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Screen confident predictions from collected CHEMBL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccab2e-8f50-46ab-b726-3c997890ce1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "uncertainty_lower_bound, uncertainty_upper_bound, dropout_times, seed = 0, 0.047, 10, 294\n",
    "# 指定目录路径\n",
    "directory = f\"/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/baseline/PARP1/infer_AFP/dropout_times={dropout_times}-seed={reed}/\"\n",
    "source_dir = '/home1/yueming/Drug_Discovery/Datasets/'\n",
    "dropout_list = [0.1, 0.2, 0.3, 0.4, 0.5] # , 0.6, 0.7, 0.8, 0.9\n",
    "dropout_column_list = [f'uncertainty-dropout={dropout}' for dropout in dropout_list]\n",
    "afse_column_list = ['AFSE_uncertainty']\n",
    "uncertainty_column_list = afse_column_list + dropout_column_list\n",
    "min_max_values = [0.20327282, 0.4505804, 0.003099999, 0.23411462, 0.00621228, 0.49756283, 0.010164783, 1.2705135, 0.00972938, 2.0085182, 0.027138822, 1.8844687]\n",
    "\n",
    "def min_max_scaling(column, min_value, max_value):\n",
    "    return (column - min_value) / (max_value - min_value)\n",
    "\n",
    "def sum_up_uncertainty(data, uncertainty_column_list, min_max_values, file_path):\n",
    "    begin, counter = True, 0\n",
    "    for i, uncertainty_column in enumerate(uncertainty_column_list):\n",
    "        if uncertainty_column not in data.columns:\n",
    "            # The column exists in the DataFrame\n",
    "            print(f\"Column {uncertainty_column} is missing in {file_path}\")\n",
    "            continue\n",
    "        min_value, max_value = min_max_values[2*i], min_max_values[2*i+1]\n",
    "        if begin:\n",
    "            z = min_max_scaling(data[uncertainty_column].values, min_value, max_value)\n",
    "            begin = False\n",
    "        else:\n",
    "            z += min_max_scaling(data[uncertainty_column].values, min_value, max_value)\n",
    "        counter += 1\n",
    "    z /= counter\n",
    "    data['sum_up_uncertainty'] = z\n",
    "\n",
    "def search_and_add_assays(selected_rows, target, assay):\n",
    "    source_path = f'{source_dir}/{target}/{assay}/{target}_{assay}_all.csv'\n",
    "    source_df = pd.read_csv(source_path)\n",
    "    source_df = source_df.rename(columns={'SMILES': 'smiles'})\n",
    "    selected_rows = selected_rows.merge(source_df, how='left', on='smiles')\n",
    "    return selected_rows\n",
    "    \n",
    "# 创建一个空的DataFrame来存储结果\n",
    "result_df = pd.DataFrame()\n",
    "# 循环遍历目录中的所有.csv文件\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        last_names = filename.split('/')[-1][:-8]\n",
    "        target = last_names.split('-')[0]\n",
    "        assay = last_names[len(target) + 1:]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        sum_up_uncertainty(df, uncertainty_column_list, min_max_values, file_path)\n",
    "        # 选择\"uncertainty\"列小于或等于uncerternty_threshold的行\n",
    "        selected_rows = df[df[\"sum_up_uncertainty\"] <= uncertainty_upper_bound]\n",
    "        selected_rows = selected_rows[selected_rows[\"sum_up_uncertainty\"] >= uncertainty_lower_bound]\n",
    "        if len(selected_rows) > 0:\n",
    "            selected_rows = search_and_add_assays(selected_rows, target, assay)\n",
    "            # 将选定的行添加到结果DataFrame\n",
    "            result_df = pd.concat([result_df, selected_rows], ignore_index=True)\n",
    "\n",
    "# 保存结果DataFrame为CSV文件\n",
    "result_file_path = f\"/home1/yueming/Drug_Discovery/Baselines/GIGN-main/GIGN/baseline/PARP1/infer_AFP/dropout_times={dropout_times}-seed={seed}-uncertainty_range={uncerternty_lower_bound}~{uncerternty_upper_bound}.csv\"\n",
    "result_df.to_csv(result_file_path, index=False)\n",
    "print(f'Screen out {len(result_df)} samples in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37fee5",
   "metadata": {},
   "source": [
    "## Convert Isomeric SMILES to Canonical SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# 输入Isomeric SMILES\n",
    "isomeric_smiles = \"c1cc2c(c(c1)NC(=O)CN3CCN(CC3)C(=O)[C@@H]4[C@H]([C@H]([C@@H](O4)n5cnc6c5ncnc6N)O)O)CNC2=O\"\n",
    "\n",
    "# 使用RDKit将Isomeric SMILES转换为Molecule对象\n",
    "mol = Chem.MolFromSmiles(isomeric_smiles)\n",
    "\n",
    "if mol is not None:\n",
    "    # 将Molecule对象转换为Canonical SMILES\n",
    "    canonical_smiles = Chem.MolToSmiles(mol, isomericSmiles=False)  # 使用isomericSmiles=False以获取Canonical SMILES\n",
    "\n",
    "    # 打印Canonical SMILES\n",
    "    print(\"Canonical SMILES:\", canonical_smiles)\n",
    "else:\n",
    "    print(\"Invalid SMILES input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646dd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
